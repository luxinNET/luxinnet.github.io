---
layout: post
title: 《大模型应用开发极简入门》笔记——discard
category: 大模型应用开发极简入门
tags: [大模型, 大模型应用]
comments: true
---

术语使用**加粗**表示

术语解释使用
> 描述
表示

## 第一章 初始GPT-4和ChatGPT

**人工智能(artificial intelligence)**
> 简称AI，是此类技术的统称。

**大语言模型(large language model, LLM)**
> 简称LLM，是参数较多的语言处理模型，而GPT-4和其他GPT模型就是LLM。


**自然语言处理(natural language processing, NLP)**
> 是人工智能(AI)和语言学领域的分支学科。
> 现代NLP解决方案基于ML算法。

**机器学习(machine learning, ML)**
> 是人工智能领域一个重要的分支

**深度学习(deep learning, DL)**
> 是ML的一个分支，专注于受大脑结构启发的算法。

**人工神经网络(artificial neural network)**
> DL和ML等算法被称为人工神经网络。

**Transformer(一种特定神经网络架构)**
> 像阅读机一样，只关注句子或段落的不同部分，以理解其上下文并产生连贯的回答。

**词频**
> n-gram模型通过使用词频来根据前面的词预测句子中的下一个词。词频指的是在训练文本中紧随前面的词出现的频率最高的词。

**循环神经网络(recurrent neural network, RNN)**
> 是神经网络的一种。

**长短期记忆(long short-term memory, LSTM)网络**
> 是一种时间循环神经网络。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。

**注意力机制**
> Transformer架构的核心支柱是注意力机制。

**交叉注意力、自注意力**
> 交叉注意力和自注意力是基于注意力机制的两个架构模块，他们经常出现在LLM中。Transformer架构广泛使用了这两个模块。

**图形处理单元(graphics processing unit, GPU)**
> 是计算设备中的处理单元，用于在硬件设备上实现3D图像、高清视频流、图形等技术概念。

**嵌入(embedding)**
> 标准的Transformer架构有两个主要组件：编码器和解码器，两者都十分依赖注意力机制。
> 编码器的任务是处理输入文本，识别有价值的特征，并生成有意义的文本表示，称为嵌入。
> 解码器使用这个嵌入来生成一个输出，比如翻译结果或摘要文本。

**生成式预训练Transformer(Generative Pre-trained Transformer, GPT)**
> 是一类基于Transformer架构的模型，专门利用原始架构中的解码器部分。

**文本补全**
> GPT接收一段提示词作为输入，然后生成一段文本作为输出。这个过程被称为文本补全。

**标记(token)**
> 当GPT模型收到一段提示词之后，它首先讲输入拆分成标记。
> 这些标记代表单词、单词的一部分、空格或标点符号。

**提示工程(prompt engineering)**

**监督微调(supervised fine-tuning, SFT)**

**通过人类反馈进行强化学习(reinforcement learning from human feedback, RLHF)**

**温度(temperature)**

**基础模型**

### 1.1 LLM概述

GPT-4和ChatGPT基于一种特定的神经网络架构，即Transformer。Transformer像阅读机一样，只关注句子或段落的不同部分，以理解其上下文并产生连贯的回答，此外还可以理解句子中的单词顺序和上下文意思。

人工智能AI 包含 机器学习ML 包含 深度学习DL 

Transformer架构使用深度学习技术，而GPT-4和ChatGPT基于此。

NLP的目标是让计算机能够处理自然语言文本。这个目标涉及诸多任务，如下所述

文本分类、自动翻译、问题回答、文本生成

单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合LSTM可以很好的解决这个问题。时间循环神经网络可以描述动态时间行为，因为和前馈神经网络接受特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。

Transformer架构彻底改变了NLP领域，这要是因为它能有效地解决之前NLP存在的一个问题：很难处理长文本序列并记住其上下文。

LLM是试图完成文本生成任务的一类ML模型。

交叉注意力模块会更关注句子中的个别关键部分，有助于模型为句子的这一部分生成准确的翻译结果。

与RNN不同，Transformer架构具有易于并行化的优势，这意味着Transformer架构可以同时处理输入文本的多个部分，而无需顺序处理。

与之前的循环模型不同，带有注意力机制的Transformer架构使得LLM能够将上下文作为一个整体来考虑。

1. 收到提示词
2. 将输入拆分为标记
3. 采用Transformer架构处理标记
4. 基于上下文预测下一个标记
5. 根据概率分数选择标记

逐个标记地补全文本，整个过程是迭代式的

### 1.2 GPT模型简史：从GPT-1到GPT-4

Transformer架构诞生于2017年中，一年后，OpenAI发表了一篇介绍GPT-1的论文。

OpenAI发展时间表：
* 2017年：Ashish Vaswani等人发表论文"Attention Is All You Need"
* 2018年：第一个GPT模型诞生，参数量为1.17亿
* 2019年：GPT-2模型发布，参数量为15亿
* 2020年：GPT-3模型发布，参数量为1750亿
* 2022年：GPT-3.5（ChatGPT）模型发布，参数量为1750亿
* 2023年：GPT-4模型发布，但具体的参数量未公开

## 第二章 深入了解GPT-4和ChatGPT的API

## 第三章 使用GPT-4和ChatGPT构建应用程序

## 第四章 GPT-4和ChatGPT的高级技巧

## 第五章 使用LangChain框架合插件增强LLM的功能
